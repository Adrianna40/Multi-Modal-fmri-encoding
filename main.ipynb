{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from data_load import get_subj_dataset\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader\n",
    "from image_processing import ImageDataset, fit_pca, extract_features_with_pca\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from text_processing import get_embeddings, get_coco_info\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from evaluation import calculate_corr, get_roi_corr, plot_corr_all_rois\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1\n",
    "batch_size = 100\n",
    "n_components = 100\n",
    "rand_seed = 5 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nsd_stim_info_file_path = 'nsd_stim_info_merged.csv'\n",
    "coco_annotation_path = 'annotations'\n",
    "data_dir = 'algonauts_2023_challenge_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_paths, subj_data = get_subj_dataset(subj, data_dir)\n",
    "\n",
    "num_train = int(np.round(len(subj_paths.img_list) * 0.9))\n",
    "\n",
    "idxs = np.arange(len(subj_paths.img_list))\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "idxs_test = np.arange(len(subj_paths.img_list_test))\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "print('\\nTest stimulus images: ' + format(len(idxs_test)))\n",
    "\n",
    "lh_fmri_train = subj_data.lh_fmri[idxs_train]\n",
    "rh_fmri_train = subj_data.rh_fmri[idxs_train]\n",
    "lh_fmri_val = subj_data.lh_fmri[idxs_val]\n",
    "rh_fmri_val = subj_data.rh_fmri[idxs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image features \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "train_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(subj_paths.img_dir_list, idxs_train, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(subj_paths.img_dir_list, idxs_val, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_imgs_dataloader = DataLoader(\n",
    "    ImageDataset(subj_paths.img_dir_list_test, idxs_test, transform), \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet')\n",
    "model.to(device) \n",
    "model.eval()\n",
    "model_layer = \"features.5\"\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=[model_layer])\n",
    "pca = fit_pca(feature_extractor, train_imgs_dataloader)\n",
    "features_train = extract_features_with_pca(feature_extractor, train_imgs_dataloader, pca)\n",
    "features_val = extract_features_with_pca(feature_extractor, val_imgs_dataloader, pca)\n",
    "features_test = extract_features_with_pca(feature_extractor, test_imgs_dataloader, pca)\n",
    "del model, pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text features \n",
    "model_checkpoint = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
    "model = BertForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "nsd_to_coco, coco_id_to_description = get_coco_info()\n",
    "train_emb = get_embeddings(subj_paths.img_dir_list, idxs_train, model, tokenizer)\n",
    "val_emb = get_embeddings(subj_paths.img_dir_list, idxs_val, model, tokenizer) \n",
    "test_emb = get_embeddings(subj_paths.img_dir_list_test, idxs_test, model, tokenizer) \n",
    "del model, tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features in one model \n",
    "\n",
    "X_train = np.hstack([train_emb, features_train])\n",
    "X_val = np.hstack([val_emb, features_val])\n",
    "\n",
    "reg_lh = LinearRegression().fit(X_train, lh_fmri_train)\n",
    "reg_rh = LinearRegression().fit(X_train, rh_fmri_train)\n",
    "\n",
    "lh_fmri_val_pred = reg_lh.predict(X_val)\n",
    "rh_fmri_val_pred = reg_rh.predict(X_val)\n",
    "lh_corr = calculate_corr(lh_fmri_val_pred, lh_fmri_val)\n",
    "rh_corr = calculate_corr(rh_fmri_val_pred, rh_fmri_val)\n",
    "plot_corr_all_rois(lh_corr, rh_corr, f'1 model with text and image features')\n",
    "get_roi_corr(lh_corr, rh_corr)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
